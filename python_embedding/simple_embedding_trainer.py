import pandas as pd
import numpy as np
from gensim.models import Word2Vec
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
import logging
import sys

# è®¾ç½®æ—¥å¿—
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

def train_item2vec_large_dataset():
    """è®­ç»ƒå¤§æ•°æ®é›†çš„Item2Vecæ¨¡å‹"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒå¤§æ•°æ®é›†Item2Vecæ¨¡å‹...")

    # è¯»å–æ•°æ®
    print("ğŸ“– è¯»å–è¯„åˆ†æ•°æ®...")
    ratings = pd.read_csv('ml-25m/ratings.csv')
    print(f"æ€»è¯„åˆ†æ•°: {len(ratings):,}")
    print(f"ç”¨æˆ·æ•°: {ratings['userId'].nunique():,}")
    print(f"ç”µå½±æ•°: {ratings['movieId'].nunique():,}")

    # æ•°æ®é¢„å¤„ç† - ç­›é€‰æ´»è·ƒç”¨æˆ·å’Œçƒ­é—¨ç”µå½±
    print("ğŸ”„ æ•°æ®é¢„å¤„ç†...")

    # ç”¨æˆ·è¯„åˆ†ç»Ÿè®¡
    user_counts = ratings['userId'].value_counts()
    movie_counts = ratings['movieId'].value_counts()

    # ç­›é€‰æ ‡å‡†: ç”¨æˆ·è‡³å°‘20ä¸ªè¯„åˆ†ï¼Œç”µå½±è‡³å°‘50ä¸ªè¯„åˆ†
    active_users = user_counts[user_counts >= 20].index
    popular_movies = movie_counts[movie_counts >= 50].index

    # ç­›é€‰æ•°æ®
    filtered_ratings = ratings[
        (ratings['userId'].isin(active_users)) &
        (ratings['movieId'].isin(popular_movies))
    ].copy()

    print(f"ç­›é€‰åè¯„åˆ†æ•°: {len(filtered_ratings):,}")
    print(f"ç­›é€‰åç”¨æˆ·æ•°: {filtered_ratings['userId'].nunique():,}")
    print(f"ç­›é€‰åç”µå½±æ•°: {filtered_ratings['movieId'].nunique():,}")

    # æŒ‰æ—¶é—´æ’åºï¼Œä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºç”µå½±åºåˆ—
    print("ğŸ“ åˆ›å»ºç”¨æˆ·-ç”µå½±åºåˆ—...")
    filtered_ratings = filtered_ratings.sort_values(['userId', 'timestamp'])

    # åˆ›å»ºç”¨æˆ·åºåˆ—
    user_sequences = []
    for user_id in filtered_ratings['userId'].unique():
        user_movies = filtered_ratings[filtered_ratings['userId'] == user_id]['movieId'].astype(str).tolist()
        if len(user_movies) >= 5:  # è‡³å°‘5ä¸ªç”µå½±çš„åºåˆ—
            user_sequences.append(user_movies)

    print(f"åˆ›å»ºäº† {len(user_sequences):,} ä¸ªç”¨æˆ·åºåˆ—")

    # è®­ç»ƒWord2Vecæ¨¡å‹
    print("ğŸ¤– è®­ç»ƒWord2Vecæ¨¡å‹...")
    model = Word2Vec(
        sentences=user_sequences,
        vector_size=128,        # æ›´å¤§çš„å‘é‡ç»´åº¦
        window=5,
        min_count=5,           # æœ€å°‘å‡ºç°5æ¬¡
        workers=4,
        epochs=10,
        sg=1                   # Skip-gram
    )

    # ä¿å­˜ç”µå½±embeddings
    print("ğŸ’¾ ä¿å­˜ç”µå½±embeddings...")
    item_embeddings = []
    for movie_id in model.wv.index_to_key:
        embedding = ' '.join(map(str, model.wv[movie_id]))
        item_embeddings.append(f"{movie_id}:{embedding}")

    with open('../src/main/resources/webroot/modeldata/item2vecEmb_large.csv', 'w') as f:
        f.write('\n'.join(item_embeddings))

    print(f"âœ… ä¿å­˜äº† {len(item_embeddings):,} ä¸ªç”µå½±embeddings")
    return model, filtered_ratings

def train_user_embeddings_large_dataset(filtered_ratings, item_model):
    """è®­ç»ƒå¤§æ•°æ®é›†çš„ç”¨æˆ·embeddings"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒç”¨æˆ·embeddings...")

    # è·å–ç”¨æˆ·çš„ç”µå½±åå¥½å‘é‡
    print("ğŸ“Š è®¡ç®—ç”¨æˆ·åå¥½å‘é‡...")

    # åˆ›å»ºç”¨æˆ·-ç”µå½±çŸ©é˜µ
    user_movie_matrix = filtered_ratings.pivot_table(
        index='userId',
        columns='movieId',
        values='rating',
        fill_value=0
    )

    print(f"ç”¨æˆ·-ç”µå½±çŸ©é˜µå¤§å°: {user_movie_matrix.shape}")

    # ä½¿ç”¨SVDé™ç»´
    print("ğŸ” ä½¿ç”¨SVDè¿›è¡Œé™ç»´...")
    svd = TruncatedSVD(n_components=128, random_state=42)
    user_embeddings_matrix = svd.fit_transform(user_movie_matrix)

    # ä¿å­˜ç”¨æˆ·embeddings
    print("ğŸ’¾ ä¿å­˜ç”¨æˆ·embeddings...")
    user_embeddings = []
    for i, user_id in enumerate(user_movie_matrix.index):
        embedding = ' '.join(map(str, user_embeddings_matrix[i]))
        user_embeddings.append(f"user_{i}:{embedding}")

    with open('../src/main/resources/webroot/modeldata/userEmb_large.csv', 'w') as f:
        f.write('\n'.join(user_embeddings))

    print(f"âœ… ä¿å­˜äº† {len(user_embeddings):,} ä¸ªç”¨æˆ·embeddings")

def main():
    print("=" * 60)
    print("ğŸ¬ SparrowRecSys å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒ")
    print("=" * 60)

    try:
        # è®­ç»ƒItem2Vec
        item_model, filtered_ratings = train_item2vec_large_dataset()

        # è®­ç»ƒç”¨æˆ·embeddings
        train_user_embeddings_large_dataset(filtered_ratings, item_model)

        print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print("ç”Ÿæˆçš„æ–‡ä»¶:")
        print("  - item2vecEmb_large.csv: ç”µå½±embeddings")
        print("  - userEmb_large.csv: ç”¨æˆ·embeddings")

    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()